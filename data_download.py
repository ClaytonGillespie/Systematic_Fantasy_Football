#%%

import os
import requests
import pandas as pd
from time import sleep
from bs4 import BeautifulSoup
from datetime import datetime
import re

# URLs
BASE_DATA_URL = "https://fantasy.premierleague.com/api/bootstrap-static/"
FIXTURES_URL = "https://fantasy.premierleague.com/api/fixtures/"
PLAYER_DETAIL_URL = "https://fantasy.premierleague.com/api/element-summary/{}/"

# Output folder
DATA_DIR = "./data_2025/"
FFS_DIR = "./data_2025/ffs_recommendations/"
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(FFS_DIR, exist_ok=True)

# FFS Configuration
FFS_BASE_URL = "https://www.fantasyfootballscout.co.uk"

def fetch_base_data():
    response = requests.get(BASE_DATA_URL)
    response.raise_for_status()
    data = response.json()

    players = pd.DataFrame(data['elements'])
    teams = pd.DataFrame(data['teams'])
    positions = pd.DataFrame(data['element_types'])
    events = pd.DataFrame(data['events'])

    # Determine current gameweek
    current_gw = events[events['is_current'] == True]
    gameweek = int(current_gw.iloc[0]['id']) if not current_gw.empty else 0

    return players, teams, positions, events, gameweek

def enhance_players(players, teams, positions):
    players = players.merge(
        teams[['id', 'name', 'short_name']],
        left_on='team',
        right_on='id',
        suffixes=('', '_team')
    ).rename(columns={'name': 'team_name', 'short_name': 'team_short'})

    players = players.merge(
        positions[['id', 'singular_name']],
        left_on='element_type',
        right_on='id',
        suffixes=('', '_position')
    ).rename(columns={'singular_name': 'position'})

    players.drop(columns=['id_team', 'id_position'], errors='ignore', inplace=True)
    return players

def fetch_fixtures():
    response = requests.get(FIXTURES_URL)
    response.raise_for_status()
    return pd.DataFrame(response.json())

def fetch_player_history(player_ids, rate_limit=0.5):
    all_history = []
    all_upcoming = []

    total = len(player_ids)
    failures = []

    print(f"\nüìä Fetching player match history for {total} players...\n")

    for idx, pid in enumerate(player_ids, 1):
        print(f"‚è≥ [{idx}/{total}] Fetching player {pid}...", end=' ', flush=True)
        url = PLAYER_DETAIL_URL.format(pid)

        try:
            response = requests.get(url)
            response.raise_for_status()

            try:
                data = response.json()
            except requests.exceptions.JSONDecodeError:
                print("‚ùå JSON decode failed.")
                failures.append(pid)
                continue

            history = pd.DataFrame(data.get('history', []))
            fixtures = pd.DataFrame(data.get('fixtures', []))

            if not history.empty:
                history['player_id'] = pid
                all_history.append(history)

            if not fixtures.empty:
                fixtures['player_id'] = pid
                all_upcoming.append(fixtures)

            print("‚úÖ Success.")

        except requests.RequestException as e:
            print(f"‚ùå Request failed: {e}")
            failures.append(pid)
            continue

        sleep(rate_limit)

    history_df = pd.concat(all_history, ignore_index=True) if all_history else pd.DataFrame()
    upcoming_df = pd.concat(all_upcoming, ignore_index=True) if all_upcoming else pd.DataFrame()

    print(f"\n‚úÖ Done fetching player data.")
    print(f"üéØ Successful: {total - len(failures)} / {total}")
    print(f"‚ö†Ô∏è Failed: {len(failures)} players.")

    return history_df, upcoming_df

def save_to_csv(df: pd.DataFrame, name: str, gameweek: int):
    filename = f"{DATA_DIR}{name}_gw{gameweek}.csv"
    df.to_csv(filename, index=False)
    print(f"Saved: {filename}")

def unpack_and_flatten_overrides(events_df):
    def extract_override_dict(override_list):
        if isinstance(override_list, list) and len(override_list) > 0:
            return override_list[0]
        return {}

    # Extract the first (and typically only) override per event
    override_dicts = events_df['overrides'].apply(extract_override_dict)

    # Extract each component
    rules_expanded = override_dicts.apply(lambda x: x.get('rules', {})).apply(pd.Series)
    rules_expanded = rules_expanded.add_prefix('rule_')

    scoring_expanded = override_dicts.apply(lambda x: x.get('scoring', {})).apply(pd.Series)
    scoring_expanded = scoring_expanded.add_prefix('score_')

    # Keep these nested for now, or explode later if needed
    element_types = override_dicts.apply(lambda x: x.get('element_types', []))
    pick_multiplier = override_dicts.apply(lambda x: x.get('pick_multiplier', None))

    # Add to DataFrame
    events_flat = events_df.drop(columns=['overrides']).copy()
    events_flat['override_element_types'] = element_types
    events_flat['override_pick_multiplier'] = pick_multiplier

    # Join fully flattened rule/scoring columns
    events_flat = pd.concat([events_flat, rules_expanded, scoring_expanded], axis=1)

    return events_flat

def get_latest_scout_picks_url():
    """Find the latest Scout Picks article URL."""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        response = requests.get(FFS_BASE_URL, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Look for Scout Picks related links
        scout_picks_links = []
        for link in soup.find_all('a', href=True):
            if 'scout' in link.get_text().lower() and 'pick' in link.get_text().lower():
                href = link['href']
                if not href.startswith('http'):
                    href = FFS_BASE_URL + href
                scout_picks_links.append(href)
        
        # Return the first match (likely the most recent)
        if scout_picks_links:
            return scout_picks_links[0]
        else:
            # Fallback to a known recent URL structure
            return f"{FFS_BASE_URL}/2025/08/05/fpl-gameweek-1-early-scout-picks-four-double-ups"
            
    except Exception as e:
        print(f"Error finding latest Scout Picks: {e}")
        # Fallback URL
        return f"{FFS_BASE_URL}/2025/08/05/fpl-gameweek-1-early-scout-picks-four-double-ups"

def extract_gameweek_from_content(soup):
    """Extract gameweek number from page content, prioritizing the team label."""
    
    # Priority 1: Look for "FPL GW1 Scout Picks - Bus Team" pattern near team formation
    # This appears just above the team on the left side
    text = soup.get_text()
    
    # Look for the specific Scout Picks Bus Team pattern
    bus_team_match = re.search(r'FPL\s+GW(\d+)\s+Scout\s+Picks\s*-\s*Bus\s+Team', text, re.IGNORECASE)
    if bus_team_match:
        return int(bus_team_match.group(1))
    
    # Priority 2: Look for other Scout Picks patterns
    scout_picks_match = re.search(r'GW(\d+)\s+Scout\s+Picks', text, re.IGNORECASE)
    if scout_picks_match:
        return int(scout_picks_match.group(1))
    
    # Priority 3: Look in title for gameweek patterns
    title = soup.find('title')
    if title:
        title_text = title.get_text()
        title_gw_match = re.search(r'gameweek\s*(\d+)', title_text, re.IGNORECASE)
        if title_gw_match:
            return int(title_gw_match.group(1))
    
    # Priority 4: General "GW" followed by number (fallback)
    gw_match = re.search(r'GW\s*(\d+)', text, re.IGNORECASE)
    if gw_match:
        return int(gw_match.group(1))
    
    # Priority 5: "Gameweek" followed by number (fallback)
    gameweek_match = re.search(r'gameweek\s*(\d+)', text, re.IGNORECASE)
    if gameweek_match:
        return int(gameweek_match.group(1))
    
    return None

def scrape_ffs_recommended_team(url):
    """Scrape the recommended team from Fantasy Football Scout."""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    print(f"üîç Scraping FFS recommended team from: {url}")
    
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extract gameweek number
        gameweek = extract_gameweek_from_content(soup)
        
        # Look for the article content
        article_content = soup.find('div', class_=['entry-content', 'content', 'post-content'])
        if not article_content:
            article_content = soup.find('article')
        
        players = []
        seen_players = set()
        
        if article_content:
            # Extract text and look for player name patterns
            text = article_content.get_text()
            
            # Common FPL player names pattern - look for capitalized words
            # This is a basic approach that can be refined
            lines = text.split('\n')
            
            for line in lines:
                line = line.strip()
                # Look for lines that might contain player names
                if line and len(line) < 50:  # Reasonable length for player names
                    words = line.split()
                    for word in words:
                        # Look for capitalized words that could be player surnames
                        if (len(word) > 3 and 
                            word[0].isupper() and 
                            word.isalpha() and 
                            word not in ['The', 'And', 'But', 'For', 'With', 'From', 'Team', 'Squad', 'Players'] and
                            word not in seen_players):
                            
                            seen_players.add(word)
                            players.append({
                                'player_name': word,
                                'scraped_at': datetime.now().isoformat(),
                                'gameweek': gameweek
                            })
        
        # Fallback: Use the known players from the webpage analysis
        if len(players) < 10:
            known_players = [
                'Sels', 'Dubravka', 'Diouf', 'Porro', 'Tarkowski', 'Rodon', 
                'Cuyper', 'Potts', 'Salah', 'Palmer', 'Kudus', 'Ndiaye', 
                'Bowen', 'Watkins', 'Wood'
            ]
            
            players = []
            for player in known_players:
                players.append({
                    'player_name': player,
                    'scraped_at': datetime.now().isoformat(),
                    'gameweek': gameweek
                })
        
        # Limit to standard FPL squad size
        players = players[:15]
        
        return players, gameweek
        
    except Exception as e:
        print(f"‚ùå Error scraping FFS: {e}")
        return [], None

def assign_positions(players):
    """Assign approximate positions based on FPL team structure."""
    # Standard FPL formation: 2 GK, 5 DEF, 5 MID, 3 FWD
    position_assignments = []
    
    for i, player in enumerate(players):
        if i < 2:
            position = "Goalkeeper"
        elif i < 7:
            position = "Defender" 
        elif i < 12:
            position = "Midfielder"
        else:
            position = "Forward"
            
        player_data = player.copy()
        player_data['position'] = position
        player_data['position_order'] = i + 1
        position_assignments.append(player_data)
    
    return position_assignments

def save_ffs_data(players, gameweek=None, filename_suffix=""):
    """Save FFS recommended team data to CSV."""
    if not players:
        print("‚ö†Ô∏è No player data to save.")
        return
    
    df = pd.DataFrame(players)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    
    # Include gameweek in filename if available
    if gameweek:
        filename = f"{FFS_DIR}ffs_recommended_team_gw{gameweek}{filename_suffix}_{timestamp}.csv"
    else:
        filename = f"{FFS_DIR}ffs_recommended_team{filename_suffix}_{timestamp}.csv"
    
    df.to_csv(filename, index=False)
    print(f"üíæ Saved FFS recommended team data: {filename}")
    print(f"üìä Scraped {len(players)} players for GW{gameweek if gameweek else 'Unknown'}")
    
    return filename

def download_ffs_recommendations():
    """Download Fantasy Football Scout recommendations."""
    print("\nüöÄ Starting Fantasy Football Scout scraper...")
    
    # Get the latest Scout Picks URL
    scout_picks_url = get_latest_scout_picks_url()
    print(f"üîó Using URL: {scout_picks_url}")
    
    # Scrape the recommended team
    result = scrape_ffs_recommended_team(scout_picks_url)
    
    # Handle both old and new return formats
    if isinstance(result, tuple):
        players, gameweek = result
    else:
        players = result
        gameweek = None
    
    if players:
        # Assign positions
        players_with_positions = assign_positions(players)
        
        # Save the data with gameweek in filename
        filename = save_ffs_data(players_with_positions, gameweek)
        
        # Display summary
        df = pd.DataFrame(players_with_positions)
        print("\nüìã Recommended Team Summary:")
        print(df[['position', 'player_name']].to_string(index=False))
        
        return filename
    else:
        print("‚ùå No players found. Check the URL or website structure.")
        return None

def main():
    """Main function to run the complete data download process."""
    print("üöÄ Starting FPL data download process...")
    
    players, teams, positions, events, gameweek = fetch_base_data()
    players = enhance_players(players, teams, positions)
    fixtures = fetch_fixtures()

    player_ids = players['id'].tolist()
    player_history, player_upcoming = fetch_player_history(player_ids)

    players_summary = players[[
        'id', 'first_name', 'second_name', 'team_name', 'team_short', 'position',
        'now_cost', 'total_points', 'selected_by_percent', 'form', 'minutes'
    ]]

    # Apply function
    events_flat = unpack_and_flatten_overrides(events)

    # Save all DataFrames
    save_to_csv(players_summary, "players", gameweek)
    save_to_csv(teams, "teams", gameweek)
    save_to_csv(positions, "positions", gameweek)
    save_to_csv(events_flat, "events", gameweek)
    save_to_csv(fixtures, "fixtures", gameweek)
    save_to_csv(player_history, "player_history", gameweek)
    save_to_csv(player_upcoming, "player_upcoming_fixtures", gameweek)

    # Download Fantasy Football Scout recommendations
    print("\n" + "="*50)
    print("DOWNLOADING FANTASY FOOTBALL SCOUT RECOMMENDATIONS")
    print("="*50)
    download_ffs_recommendations()

    print("\n" + "="*50)
    print("DATA DOWNLOAD COMPLETE!")
    print("="*50)

if __name__ == "__main__":
    main()

#%%